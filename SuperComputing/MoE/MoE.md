# MoE

## FastMoE

### [FastMoE开源大规模分布式MoE训练框架](https://www.bilibili.com/video/BV1pg411M7mV/?p=4&vd_source=2ef7e92f2d522c31939f486aea77a19e)

### [智源x清华开源FastMoE，万亿AI模型基石](https://zhuanlan.zhihu.com/p/354647536)


### [github:laekov/fastmoe](https://github.com/laekov/fastmoe)

### [FastMoE开源分布式MoE模型训练系统](https://zhuanlan.zhihu.com/p/399496787)

## 八卦炉

### [「八卦炉」炼丹规模直逼人脑！清华、阿里等搞了个174万亿参数大模型](https://cloud.tencent.com/developer/article/1955751)

### [BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores](https://keg.cs.tsinghua.edu.cn/jietang/publications/PPOPP22-Ma%20et%20al.-BaGuaLu%20Targeting%20Brain%20Scale%20Pretrained%20Models%20w.pdf)

### [MEET2023智能未来大会（上午场直播回放）](https://www.bilibili.com/video/BV1we411c7gr/?spm_id_from=333.337.search-card.all.click)
* 暂时做个笔记，回头细看。

