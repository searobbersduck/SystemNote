## [Power Your AI Inference with New NVIDIA Triton and NVIDIA TensorRT Features](https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/)

New features in TensorRT include multi-GPU multi-node inference, performance and hardware optimizations, and more.

* ### Multi-GPU multi-node inference
  * TensorRT can be used to run multi-GPU multi-node inference for large language models (LLMs). It supports GPT-3 175B, 530B, and 6.7B models. These models do not require ONNX conversion; rather, a simple Python API is available to optimize for multi-GPU inference. Now available in private early access. Contact your NVIDIA account team for more details. 
* ### TensorRT 8.6 
  * TensorRT 8.6 is now available in early access and includes the following key features:
    * Performance optimizations for generative AI diffusion and transformer models
    * Hardware compatibility to build and run on different GPU architectures
    * Version compatibility to build and run on different TensorRT versions
    * Optimization levels to trade between build time and inference performance


<br><br>
****
<br><br>

## [NVIDIA TensorRT](https://developer.nvidia.com/blog/sdks-accelerating-industry-5-0-data-pipelines-computational-science-and-more-featured-at-gtc-2023/) (blog time: Mar 22, 2023)

* New features:
  * Performance optimizations for generative AI diffusion and transformer models
  * Enhanced hardware compatibility to build and run on different GPU architectures
  * Version compatibility so that you can build and run on different TensorRT versions from TensorRT 8.6 and later
  * Multi-GPU, multi-node inference for GPT-3 models in early access

<br><br>
****
<br><br>

## [Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)




## todo
* [使用 Torch TensorRT 将 PyTorch 的推理速度提高6倍](https://developer.nvidia.com/zh-cn/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)
* [Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)
* [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf)
* [NVIDIA 宣布 TensorRT 8.2 以及与 PyTorch 和 TensorFlow 的集成](https://developer.nvidia.com/zh-cn/blog/nvidia-announces-tensorrt-8-2-and-integrations-with-pytorch-and-tensorflow/)
* [PyTorch 2.0 推理速度测试：与 TensorRT 、ONNX Runtime 进行对比](https://cloud.tencent.com/developer/article/2212213)
* [Exploring Next-Generation Methods for Optimizing PyTorch Models for Inference with Torch-TensorRT [S51714]](https://register.nvidia.com/flow/nvidia/gtcspring2023/attendeeportal/page/sessioncatalog/session/1666640111186001HJRI/?nvid=nv-int-bnr-463583)
* [Compile and Train with 43% Speedup using PyTorch 2.0 [S52422]](https://register.nvidia.com/flow/nvidia/gtcspring2023/attendeeportal/page/sessioncatalog/session/1674866278505001iIKa)
* [TensorRT 的最佳性能实践](https://developer.nvidia.com/zh-cn/blog/tensorrt-measuring-performance-cn/)
* [Speeding Up Deep Learning Inference Using NVIDIA TensorRT (Updated)](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt-updated/)
* [NVIDIA cuOpt](https://developer.nvidia.com/cuopt-logistics-optimization)
* [Understand BLOOM, the Largest Open-Access AI, and Run It on Your Local Computer](https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32)


## TODO

### [Colossal-AI使用指南：5分钟搭建在线OPT服务](https://colossalai.org/zh-Hans/docs/advanced_tutorials/opt_service)
### [Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models](https://arxiv.org/pdf/2302.02599.pdf)
### [NVIDIA AI 平台為大型語言模型 NeMo Megatron 框架帶來 30% 訓練速度提升](https://news.xfastest.com/nvidia/115750/nvidia-ai-nemo-megatron/)
### [Adapting P-Tuning to Solve Non-English Downstream Tasks](https://developer.nvidia.com/blog/adapting-p-tuning-to-solve-non-english-downstream-tasks/)
### [NVIDIA AI Platform Delivers Big Gains for Large Language Models](https://developer.nvidia.com/blog/nvidia-ai-platform-delivers-big-gains-for-large-language-models/)

### [Colossal AI](https://github.com/hpcaitech/ColossalAI)

### [Segment Anything](https://github.com/facebookresearch/segment-anything)

### [LLaMA-Adapter: Efficient Fine-tuning of LLaMA 🚀](https://github.com/zrrskywalker/llama-adapter)

